# -*- coding: utf-8 -*-
"""CNN_5layers_using_pure_TensorFlow.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1szA4rXI36z_qF1HeAEDcOmePfT3VzC1v

# Using just TensorFlow Framework for IRIS dataset

### Step 1: Import the required libraries
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

"""### Step 2: Load the `IRIS` dataset"""

path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv' #import IRIS dataset
df = pd.read_csv(path, header=None)

df.head(2)

headers = ["sepal_length", "sepal_width", "petal_length", "petal_width", "species"]
### Assign the headers to `df`
df.columns = headers

df.sample(2)

### Import SKlearn libraries
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

### Let us prepare the data - separating the features from the target
X, y = df.values[:,:-1], df.values[:,-1]

X[:2]

y[:2]

X = X.astype('float32')

y = LabelEncoder().fit_transform(y)

y

"""### Step 3: Split the `IRIS` dataset into `train` and `test`"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.30, random_state=42)

X_train.shape, X_test.shape

"""### Step 4: Build the neural network model"""

### Dertermine the number of features
n_features = X_train.shape[1]
print('Number of features:',n_features)

### Determine the number of classes
n_classes = len(np.unique(y))
print('Number of classes:', n_classes)

"""#### Step 4A: Define the CNN model

**`CNN:`Convoluted Neural Network**

*   initialize the weights and biases for each layer
*   implement the forward pass

"""

class CNNModel(tf.Module):
  def __init__(self):#initializes weights and biases for each layer

    ### represents weight and bias for the first layer
    ### creating a TF variable 'w1' with dimensions: n_features, 20 neurons and initializing w1 with random values drawn
    ## from a normal distribution having std. deviaton of 0.1
    self.w1 = tf.Variable(tf.random.normal([n_features, 20], stddev=0.1), name='w1')
    self.b1 = tf.Variable(tf.zeros([20]), name='b1')

    ###  Initializing weights 'w2' and bias 'b2`while connecting 20 neurons from the first layer to 20 in the second layer
    self.w2 = tf.Variable(tf.random.normal([20, 20], stddev=0.1), name='w2')
    self.b2 = tf.Variable(tf.zeros([20]), name='b2')

    ###  Initializing weights 'w3' and bias 'b3`while connecting 20 neurons from the second layer to 15 in the third layer
    self.w3 = tf.Variable(tf.random.normal([20, 15], stddev=0.1), name='w3')
    self.b3 = tf.Variable(tf.zeros([15]), name='b3')

    ###  Initializing weights 'w4' and bias 'b4`while connecting 15 neurons from the third layer to 8 in the forth layer
    self.w4 = tf.Variable(tf.random.normal([15, 8], stddev=0.1), name='w4')
    self.b4 = tf.Variable(tf.zeros([8]), name='b4')

    ###  Initializing weights 'w5' and bias 'b5`while connecting 8 neurons from the forth layer to n_classes [3] in the output layer
    self.w5 = tf.Variable(tf.random.normal([8, n_classes], stddev=0.1), name='w5')
    self.b5 = tf.Variable(tf.zeros([n_classes]), name='b5')

  def __call__(self, x):# to implement the forward pass

    ### represents weight and bias for the first layer
    ### ReLU (REctified Linear Unit) - an activation function
    layer1 = tf.nn.relu(tf.add(tf.matmul(x,self.w1), self.b1)) # performing a matrix multiplication between input features and the weight matrix. Also adding the bias matrix after in the next using `tf.add`
    layer2 = tf.nn.relu(tf.add(tf.matmul(layer1,self.w2), self.b2))
    output = tf.add(tf.matmul(layer2,self.w3), self.b3)
    return output

"""#### Step 4B: Instantiate the model




"""

model = CNNModel()

loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True) # Crossentropy loss for multi-class classification
optimizer = tf.optimizers.Adam() #Adam optimizer for training

"""#### Step 4C: Training Steps"""

### Training Function
@tf.function # compiles the function into a TensorFlow graph for efficiency
def train_step(x,y): #Performs one step of training
  with tf.GradientTape() as tape: #records operations for automatic differentiation
    logits = model(x)   # raw output of the model
    loss = loss_fn(y, logits) #compute the loss
  gradients = tape.gradient(loss, model.trainable_variables) #gradients of the loss w.r.t model parameters --> weights, biases
  optimizer.apply_gradients(zip(gradients,model.trainable_variables)) #updates the model parameters --> weights, biases
  return loss

"""#### Step 4D: Run the model loop"""

X_train.shape

n_epochs = 150 # no. of training epochs
batch_size = 32 # size of each training batch

for epoch in range(n_epochs): #looping over epochs
  dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=1000).batch(batch_size) # Creates a TF dataset from the training data,shuffles and bacthes it
  for batch_x, batch_y in dataset: #looping over batches
    loss = train_step(batch_x, batch_y) #refer the function train_step above
  if (epoch+1)%10 ==0:  #Print loss every 10 epochs
    print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}')

def evaluate(X,y): #function to compute accuracy on the test set
  logits = model(X) # raw outputs of the model
  predictions = tf.argmax(logits, axis=1) #convers logits into predicted classes
  accuracy = np.mean(predictions.numpy()==y) #mean accuracy
  return accuracy

test_accuracy = evaluate(X_test, y_test)
print(f'Test Accuracy %: {test_accuracy*100:.3f}')
